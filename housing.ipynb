{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HousingPricePrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "try:\n",
    "    # Load the housing data from CSV\n",
    "    df = spark.read.csv(\"housing.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Handle categorical variable: ocean_proximity\n",
    "    indexer = StringIndexer(inputCol=\"ocean_proximity\", outputCol=\"ocean_proximity_index\")\n",
    "    encoder = OneHotEncoder(inputCol=\"ocean_proximity_index\", outputCol=\"ocean_proximity_encoded\")\n",
    "\n",
    "    # Assemble features\n",
    "    feature_columns = [\"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "                       \"total_bedrooms\", \"population\", \"households\", \"median_income\",\n",
    "                       \"ocean_proximity_encoded\"]\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline(stages=[indexer, encoder, assembler])\n",
    "\n",
    "    # Fit the pipeline to the data\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    df_transformed = pipeline_model.transform(df)\n",
    "\n",
    "    # Select relevant columns for training\n",
    "    final_data = df_transformed.select(\"features\", \"median_house_value\")\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "    # Train a Linear Regression model\n",
    "    from pyspark.ml.regression import LinearRegression\n",
    "    lr = LinearRegression(featuresCol=\"features\", labelCol=\"median_house_value\")\n",
    "    lr_model = lr.fit(train_data)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    predictions = lr_model.transform(test_data)\n",
    "    predictions.select(\"features\", \"median_house_value\", \"prediction\").show(5)\n",
    "\n",
    "    # Evaluate the model\n",
    "    from pyspark.ml.evaluation import RegressionEvaluator\n",
    "    evaluator = RegressionEvaluator(labelCol=\"median_house_value\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Stop the Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
